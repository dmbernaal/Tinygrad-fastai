{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first get our data imports and actual data we will work with\n",
    "# NO PYTORCH \n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import Any, Dict, List, Literal, Optional, Sequence, Tuple, Union\n",
    "import tinygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/Users/diegomedina-bernal/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48)\n",
      "100%|██████████| 2/2 [00:00<00:00, 385.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get teh dataset first, we will not use dataloaders so we may need to build one from scratch? this should be fun who knows?\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just split for now, we know this, and x, y so we dont keep writing the names\n",
    "train, test = dsd[\"train\"], dsd[\"test\"]\n",
    "x, y = 'image', 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for dataloader to work\n",
    "from PIL import Image\n",
    "try: import accimage\n",
    "except ImportError: accimage = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pil_image(img: Any) -> bool:\n",
    "    if accimage is not None: return isinstance(img, (Image.Image, accimage.Image))\n",
    "    else: return isinstance(img, Image.Image)\n",
    "def is_numpy(img: Any) -> bool: return isinstance(img, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(pic) -> Tensor:\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to Tinygrad tensor.\n",
    "    thats right a tinygrad tensor not those damn pytorch ones\n",
    "    \"\"\"\n",
    "\n",
    "    if not (is_pil_image(pic) or _is_numpy(pic)):\n",
    "        raise TypeError(f\"pic should be PIL Image or ndarray. Got {type(pic)}\")\n",
    "\n",
    "    if _is_numpy(pic) and not _is_numpy_image(pic):\n",
    "        raise ValueError(f\"pic should be 2/3 dimensional. Got {pic.ndim} dimensions.\")\n",
    "\n",
    "    default_float_dtype = torch.get_default_dtype()\n",
    "\n",
    "    if isinstance(pic, np.ndarray):\n",
    "        # handle numpy array\n",
    "        if pic.ndim == 2:\n",
    "            pic = pic[:, :, None]\n",
    "\n",
    "        img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
    "        # backward compatibility\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.to(dtype=default_float_dtype).div(255)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    if accimage is not None and isinstance(pic, accimage.Image):\n",
    "        nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "        pic.copyto(nppic)\n",
    "        return torch.from_numpy(nppic).to(dtype=default_float_dtype)\n",
    "\n",
    "    # handle PIL Image\n",
    "    mode_to_nptype = {\"I\": np.int32, \"I;16\": np.int16, \"F\": np.float32}\n",
    "    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n",
    "\n",
    "    if pic.mode == \"1\":\n",
    "        img = 255 * img\n",
    "    img = img.view(pic.size[1], pic.size[0], F_pil.get_image_num_channels(pic))\n",
    "    # put it from HWC to CHW format\n",
    "    img = img.permute((2, 0, 1)).contiguous()\n",
    "    if isinstance(img, torch.ByteTensor):\n",
    "        return img.to(dtype=default_float_dtype).div(255)\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Okay we will slightly cheat, it seems that tinygrad doesnt have dataloaders or datasets so lets use pytorch ones for now and maybe we build our own? maybe? \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mTF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "## Okay we will slightly cheat, it seems that tinygrad doesnt have dataloaders or datasets so lets use pytorch ones for now and maybe we build our own? maybe? \n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
