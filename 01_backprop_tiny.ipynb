{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first get our data imports and actual data we will work with\n",
    "# NO PYTORCH \n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import Any, Dict, List, Literal, Optional, Sequence, Tuple, Union\n",
    "import tinygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/Users/diegomedina-bernal/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48)\n",
      "100%|██████████| 2/2 [00:00<00:00, 63.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's get teh dataset first, we will not use dataloaders so we may need to build one from scratch? this should be fun who knows?\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just split for now, we know this, and x, y so we dont keep writing the names\n",
    "train, test = dsd[\"train\"], dsd[\"test\"]\n",
    "x, y = 'image', 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for dataloader to work\n",
    "from PIL import Image\n",
    "try: import accimage\n",
    "except ImportError: accimage = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.helpers import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pil_image(img: Any) -> bool:\n",
    "    if accimage is not None: return isinstance(img, (Image.Image, accimage.Image))\n",
    "    else: return isinstance(img, Image.Image)\n",
    "def is_numpy(img: Any) -> bool: return isinstance(img, np.ndarray)\n",
    "def is_numpy_img(img: Any) -> bool: return is_numpy(img) and img.ndim in {2, 3}\n",
    "\n",
    "def get_image_num_channels(img: Tensor) -> int:\n",
    "    if img.ndim == 2:\n",
    "        return 1\n",
    "    elif img.ndim > 2:\n",
    "        return img.shape[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = train[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (28, 28))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_np = np.array(tst)\n",
    "type(tst_np), tst_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('L',)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of channels from pil image\n",
    "\"\"\"\n",
    "I decided to answer my own question (although I basically will sum up the comment of @cryptonome).\n",
    "\n",
    "Well, when it comes to PIL the options as I get it are:\n",
    "\n",
    "image.mode: returns a str containing the mode of the data read. Typical values are \"RGB\" and \"L\" for RGB and gray-scale images respectively. Modes are presented here.\n",
    "im2.info: which returns a dict containing various information about the image. This is image format specific. For jpg images for example it (possibly) contains fields with keys: dpi, jfif, jfif_density, exif etc. More information about jpg images can be found here.\n",
    "image.getbands(): which returns a tuple (even a 1 element one) containing all different channel present in the data. For a typical RGB image this would be ('R', 'G', 'B') and for a typical gray-scale image would be ('L',).\n",
    "So, judging from the above the more concise method in my opinion would be to compare image.mode against L and RGB strings to find if an image is gray-scale or not or if the number of channels (as in this question) is the main question then a simple len(image.getbands()) would do the job.\n",
    "\n",
    "Normally len(image.mode) will coincide with len(image.getbands()) and could be used in its place but since there is at least one mode YCbCr which contains 5 characters but only 3 channels (3x8-bit pixels, color video format) it's safer to use len(image.getbands()) I guess\n",
    "\"\"\"\n",
    "tst_pil = Image.fromarray(tst_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_np = tst_np[:, :, None] # cast\n",
    "tst_np = tst_np.transpose((2, 0, 1))\n",
    "tst_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(tst_np).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor.eye(3, requires_grad=True)\n",
    "y = Tensor([[2.0,0,-2.0]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB (3, 3) dtypes.half op:UnaryOps.CAST st:ShapeTracker(shape=(3, 3), views=[View((3, 3), (3, 1), 0, None)])> on GPU with grad None>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cast(dtypes.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Tensor <LB (3, 3) dtypes.float op:MovementOps.RESHAPE st:ShapeTracker(shape=(3, 3), views=[View((3, 4), (0, 0), 0, ((0, 3), (0, 1))), View((3, 3), (3, 1), 0, None)])> on GPU with grad None>,\n",
       " <Tensor <LB (1, 3) dtypes.float op:LoadOps.FROMCPU st:ShapeTracker(shape=(1, 3), views=[View((1, 3), (0, 1), 0, None)])> on GPU with grad None>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(pic) -> Tensor:\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to Tinygrad tensor.\n",
    "    thats right a tinygrad tensor not those damn pytorch ones\n",
    "    \"\"\"\n",
    "    if not (is_pil_image(pic) or is_numpy(pic)):\n",
    "        raise TypeError(f\"pic should be PIL Image or ndarray. Got {type(pic)}\")\n",
    "\n",
    "    if is_numpy(pic) and not is_numpy_img(pic):\n",
    "        raise ValueError(f\"pic should be 2/3 dimensional. Got {pic.ndim} dimensions.\")\n",
    "\n",
    "    # TOOD: need a way to grab the default dtype from the tensor module\n",
    "    # currently there is no way for this atm, lets change it \n",
    "    default_float_dtype = dtypes.float32\n",
    "\n",
    "    if isinstance(pic, np.ndarray):\n",
    "        if pic.ndim == 2: pic = pic[:, :, None] # add extra channel\n",
    "        img = Tensor(pic.transpose((2, 0, 1)), requires_grad=False)\n",
    "        return img\n",
    "\n",
    "    if accimage is not None and isinstance(pic, accimage.Image):\n",
    "        nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "        pic.copyto(nppic)\n",
    "        return Tensor(nppic, requires_grad=False).cast(dtype=default_float_dtype)\n",
    "\n",
    "    # handle PIL Image\n",
    "    mode_to_nptype = {\"I\": np.int32, \"I;16\": np.int16, \"F\": np.float32}\n",
    "    img = Tensor(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True), requires_grad=False)\n",
    "\n",
    "    if pic.mode == \"1\": img = 255 * img\n",
    "    img = img.view(pic.size[1], pic.size[0], F_pil.get_image_num_channels(pic))\n",
    "    # put it from HWC to CHW format\n",
    "    img = img.permute((2, 0, 1)).contiguous()\n",
    "    if isinstance(img, torch.ByteTensor):\n",
    "        return img.to(dtype=default_float_dtype).div(255)\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Okay we will slightly cheat, it seems that tinygrad doesnt have dataloaders or datasets so lets use pytorch ones for now and maybe we build our own? maybe? \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mTF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "## Okay we will slightly cheat, it seems that tinygrad doesnt have dataloaders or datasets so lets use pytorch ones for now and maybe we build our own? maybe? \n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [1, 2]\n",
    "t.insert(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0] + [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
